{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['barang original', 'brg diterima dgn baik, smoga awet',\n",
       "       'tv berkerja dengan baik, produk bagus dan berkualitas, hebat'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df = df['reviews'].values\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from spacy.lang.id import Indonesian\n",
    "from nltk.tag import CRFTagger\n",
    "from tqdm import tqdm\n",
    "from spacy.lang.id import Indonesian\n",
    "from spellchecker import correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case folding\n",
    "\n",
    "for i in range(len(df)):\n",
    "    # mengubah jadi lowercase\n",
    "    df[i] = df[i].lower()\n",
    "    # menghapus angka\n",
    "    df[i] = re.sub(r\"\\d+\", \"\", df[i])\n",
    "    # menghapus tanda baca\n",
    "    df[i] = df[i].translate(str.maketrans(\"\",\"\",string.punctuation)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords\n",
    "stop_factory = StopWordRemoverFactory().get_stop_words() #load defaul stopword\n",
    "more_stopword = ['mantap', 'bagus', 'kan'] #menambahkan stopword\n",
    "data = stop_factory + more_stopword #menggabungkan stopword\n",
    "\n",
    "dictionary = ArrayDictionary(data)\n",
    "str_ = StopWordRemover(dictionary)\n",
    "\n",
    "for i in range(len(df)):\n",
    "    df[i] = word_tokenize(str_.remove(df[i]))\n",
    "\n",
    "listStopword =  set(stopwords.words('indonesian'))\n",
    "for i in range(len(df)):\n",
    "    removed = []\n",
    "    for token in df[i]:\n",
    "        if token not in listStopword:\n",
    "            removed.append(token)\n",
    "    df[i] = removed   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "stemmer.stem('bekerja')\n",
    "for i in range(len(df)):\n",
    "    tokens = []\n",
    "    for kata in df[i]:\n",
    "        kata_dasar = stemmer.stem(kata)\n",
    "        tokens.append(kata_dasar)\n",
    "    df[i] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['barang', 'orisinal']),\n",
       "       list(['bug', 'terima', 'dan', 'moga', 'awet']),\n",
       "       list(['kerja', 'produk', 'kualitas', 'hebat'])], dtype=object)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spell checking\n",
    "for i in range(len(df)):\n",
    "    tokens = []\n",
    "    for kata in df[i]:\n",
    "        kata_asli = correction(kata)\n",
    "        if len(kata_asli) > 2:\n",
    "            tokens.append(kata_asli)\n",
    "    df[i] = tokens\n",
    "\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list(['barang', 'orisinal']) list(['bug', 'moga', 'awet'])]\n"
     ]
    }
   ],
   "source": [
    "# filter tag\n",
    "ct = CRFTagger()\n",
    "ct.set_model_file('data/all_indo_man_tag_corpus_model.crf.tagger')\n",
    "\n",
    "filters = ['NN', 'NNP', 'NNS', 'NNPS', 'JJ']\n",
    "tagged = ct.tag_sents(df)\n",
    "for i in range(len(df)):\n",
    "    sent = []\n",
    "    for idx, posTag in enumerate(tagged[i]):\n",
    "        kata = posTag[0]\n",
    "        tag = posTag[1]\n",
    "        if tag in filters:\n",
    "            sent.append(kata)\n",
    "    df[i] = sent\n",
    "print(df[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 46\n",
      "Number of documents: 58\n",
      "[[(0, 1), (1, 1)]]\n"
     ]
    }
   ],
   "source": [
    "bigram_t = Phrases(data, min_count=5)\n",
    "trigram_t = Phrases(bigram_t[data], min_count=5)\n",
    "for idx, d in enumerate(data):\n",
    "    for token in bigram_t[d]:\n",
    "        if '_' in token:# Token is a bigram, add to document.\n",
    "            data[idx].append(token)\n",
    "    for token in trigram_t[d]:\n",
    "        if '_' in token:# Token is a bigram, add to document.\n",
    "            data[idx].append(token)\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "# Remove rare & common tokens\n",
    "dictionary_t = Dictionary(data)\n",
    "dictionary_t.filter_extremes(no_below=2, no_above=0.90)\n",
    "#Create dictionary and corpus required for Topic Modeling\n",
    "corpus_t = [dictionary_t.doc2bow(doc) for doc in data]\n",
    "corpus_t = [t for t in corpus_t if t] # remove empty corpus\n",
    "print('Number of unique tokens: %d' % len(dictionary_t))\n",
    "print('Number of documents: %d' % len(corpus_t))\n",
    "print(corpus_t[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.105*\"barang\" + 0.095*\"cepat\" + 0.051*\"aman\" + 0.044*\"fungsi\" + 0.044*\"pokok\" + 0.042*\"selamat\" + 0.041*\"mudah\" + 0.039*\"kirim\" + 0.035*\"harga\" + 0.033*\"awet\"'),\n",
       " (1,\n",
       "  '0.078*\"kayu\" + 0.069*\"sesuai\" + 0.067*\"kualitas\" + 0.062*\"barang\" + 0.056*\"deskripsi\" + 0.053*\"paking\" + 0.050*\"gudang\" + 0.049*\"cepat\" + 0.036*\"mulus\" + 0.034*\"aman\"'),\n",
       " (2,\n",
       "  '0.116*\"cepat\" + 0.087*\"barang\" + 0.070*\"kirim\" + 0.059*\"awet\" + 0.042*\"moga\" + 0.042*\"gambar\" + 0.042*\"sesuai\" + 0.041*\"kualitas\" + 0.027*\"digital\" + 0.026*\"ramah\"')]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDAmodel_ = LdaMulticore(corpus=corpus_t, id2word=dictionary_t, num_topics=3)\n",
    "LDAmodel_.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coherence Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import gensim.corpora as corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(id2word, corpus, texts, limit=1, start=2, step=1):\n",
    "    coherence_values = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        LDAmodel_ = LdaMulticore(corpus=corpus, id2word=id2word, num_topics=num_topics)\n",
    "        cm = CoherenceModel(model=LDAmodel_, texts=texts, corpus=corpus, coherence='c_v')\n",
    "        with np.errstate(invalid='ignore'):\n",
    "            coherence_values.append(cm.get_coherence())\n",
    "    return coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start, step, limit = 2, 1, 10 # Ganti dengan berapa banyak Topic yang ingin di hitung/explore\n",
    "# coh_t, kCV = [], 5 # hati-hati sangat LAMBAT karena cross validasi pada metode yang memang tidak efisien (LDA)\n",
    "# id2word = corpora.Dictionary(data)\n",
    "\n",
    "# print('iterasi ke: ', end = ' ')\n",
    "# for i in range(kCV):\n",
    "#     print(i+1, end = ', ', flush=True)\n",
    "#     c = compute_coherence_values(id2word, corpus_t, data, limit=limit, start=start, step=step)\n",
    "#     coh_t.append(c)\n",
    "    \n",
    "# coherence_t = np.mean(np.array(coh_t), axis=0)\n",
    "# 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show graph\n",
    "# x = range(start, limit, step)\n",
    "# plt.figure(figsize=(12,10))\n",
    "# for c in coh_t:\n",
    "#     plt.plot(x, c, '--', color = 'lawngreen', linewidth = 2)\n",
    "# plt.plot(x, coherence_t, '-', color = 'black', linewidth = 5)\n",
    "# plt.xlabel(\"Num Topics\")\n",
    "# plt.ylabel(\"Coherence score\")\n",
    "# plt.legend((\"coherence_values\"), loc='best')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "from pyLDAvis.gensim_models import prepare as LDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pyLDAvis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\maula\\Documents\\Project\\uas_alpro\\topic_modeling.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/maula/Documents/Project/uas_alpro/topic_modeling.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pyLDAvis\u001b[39m.\u001b[39menable_notebook()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maula/Documents/Project/uas_alpro/topic_modeling.ipynb#X43sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m p \u001b[39m=\u001b[39m LDAvis(topic_model\u001b[39m=\u001b[39mLDAmodel_, corpus\u001b[39m=\u001b[39mcorpus_t, dictionary\u001b[39m=\u001b[39mdictionary_t)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maula/Documents/Project/uas_alpro/topic_modeling.ipynb#X43sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m p\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pyLDAvis' is not defined"
     ]
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "p = LDAvis(topic_model=LDAmodel_, corpus=corpus_t, dictionary=dictionary_t)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['barang', 'orisinal']), list(['bug', 'moga', 'awet']),\n",
       "       list(['kerja', 'produk', 'kualitas', 'hebat']),\n",
       "       list(['barang', 'harga', 'saing', 'cepat']),\n",
       "       list(['barang', 'orisinal', 'garansi', 'pasang', 'gampang', 'fungsi']),\n",
       "       list([]), list([]),\n",
       "       list(['proses', 'mik', 'kan', 'cepat', 'mengerang', 'tapis', 'hari', 'gratis', 'barang', 'mulus', 'gambar']),\n",
       "       list(['alhamdulillah', 'barang', 'kurir', 'payah']),\n",
       "       list(['mantapppp', 'proses', 'cepat', 'kirim', 'cepat'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d9bd8f91935e9d2c348631a57338525b6e0b118a4fc17d67b9c5af058138c69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
